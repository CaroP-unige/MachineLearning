# Lab 4 â€“ Neural Networks
This branch contains the code, experiments, and report for Lab 4: Neural Networks, part of the Machine Learning coursework in the Robotics Engineering degree.

*ðŸ§  Objectives*

  - Understand the use of MATLAB's Deep Learning Toolbox

  - Train and test feedforward neural networks on classification tasks

  - Implement and evaluate an autoencoder for dimensionality reduction

*ðŸ“š Benchmark Data Sources*

Suggested datasets:

  - UCI Machine Learning Repository: Iris, Wine, 20 Newsgroups, Breast Cancer Wisconsin, etc.

  - MNIST: Handwritten digit images

  - Kaggle Datasets: ML competition data and open datasets

*ðŸ”§ Task 0 â€“ Neural Networks in MATLAB*

MATLAB's Deep Learning Toolbox (previously Neural Network Toolbox) is used for the entire lab. To familiarize with it:

  - ðŸ”— Tutorial: Fit Data with a Neural Network

*ðŸ”— Task 1 â€“ Feedforward Neural Networks (Multi-Layer Perceptrons)*

  - ðŸ”— Tutorial: Classify Patterns with a Neural Network

ðŸ’¡ Use the GUI or function-based interface to experiment with:

  - Different datasets (e.g., Iris, Wine, MNIST)

  - Varying architectures (number of hidden layers and neurons)

ðŸ“Š Deliverables:

  - Confusion matrices (automatically generated by MATLAB)

  - Accuracy tables summarizing experiments with different configurations

*ðŸ”„ Task 2 â€“ Autoencoder*

Train an autoencoder using a multi-layer perceptron structure where:

  - Input layer = Output layer size

  - Hidden layer = Compressed representation (fewer neurons)

ðŸ§ª Experimental Workflow (using MNIST or similar):

  - Select 2 classes (e.g., digits "1" and "8") from MNIST

  - Create a dataset using only these classes

  - Train the autoencoder

  - Extract compressed (encoded) representations
